{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5095928",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.losses import Loss\n",
    "import time \n",
    "# from tensorflow.keras.applications import *\n",
    "# from matplotlib import plotly as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a04ec14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def conv_layer_to_weight_mats(in_layer_shape, conv_dict, conv_tensor,\n",
    "                              max_weight_convention=='all_one'):\n",
    "    '''\n",
    "    take in the shape of the incoming layer, a dict representing info about the\n",
    "    conv operation, and the weight tensor of the convolution, and return an \n",
    "    array of sparse weight matrices representing the operation. the array should\n",
    "    have a single element if layer_dict['max_pool_after']==False, but should have \n",
    "    two (one representing the action of the max pooling) otherwise.\n",
    "    for max pooling, we linearise by connecting the maxed neurons to everything\n",
    "    in their receptive field. if max_weight_convention=='all_one', all the \n",
    "    weights are one, otherwise if max_weight_convention=='one_on_n', the weights\n",
    "    are all one divided by the receptive field size\n",
    "    '''\n",
    "    # TODO: see if vectorisation will work\n",
    "    kernel_height, kernel_width, n_chan_in, n_chan_out = conv_tensor.shape\n",
    "    in_height = in_layer_shape[0]\n",
    "    in_width = in_layer_shape[1]\n",
    "    assert (kernel_height, kernel_width) == tuple(conv_dict['kernel_size']), f\"weight tensor info doesn't match conv layer dict info - kernel size from conv_tensor.shape is {(kernel_height, kernel_width)}, but conv_dict says it's {conv_dict['kernel_size']}\"\n",
    "    assert n_chan_out == conv_dict['filters'], f\"weight tensor info doesn't match conv layer dict info: weight tensor says num channels out is {n_chan_out}, conv dict says it's {conv_dict['filters']}\"\n",
    "    assert in_layer_shape[2] == n_chan_in, f\"weight tensor info doesn't match previous layer shape: weight tensor says it's {n_chan_in}, prev layer says it's {in_layer_shape[2]}\"\n",
    "\n",
    "    kernel_height_centre = int((kernel_height - 1) / 2)\n",
    "    kernel_width_centre = int((kernel_width - 1) / 2)\n",
    "\n",
    "    in_layer_size = np.product(in_layer_shape)\n",
    "    out_layer_shape = (in_height, in_width, n_chan_out)\n",
    "    out_layer_size = np.product(out_layer_shape)\n",
    "\n",
    "    conv_weight_matrix = np.zeros((in_layer_size, out_layer_size))\n",
    "\n",
    "    # THIS WORKS ONLY FOR SAME and not for VALID!!!\n",
    "    for i in range(in_height):\n",
    "        for j in range(in_width):\n",
    "            for c_out in range(n_chan_out):\n",
    "                out_int = cnn_layer_tup_to_int((i,j,c_out), out_layer_shape)\n",
    "                for n in range(kernel_height):\n",
    "                    for m in range(kernel_width):\n",
    "                        for c_in in range(n_chan_in):\n",
    "                            weight = conv_tensor[n][m][c_in][c_out]\n",
    "                            h_in = i + n - kernel_height_centre\n",
    "                            w_in = j + m - kernel_width_centre\n",
    "                            in_bounds_check = (h_in in range(in_height)\n",
    "                                               and w_in in range(in_width))\n",
    "                            if in_bounds_check:\n",
    "                                in_int = cnn_layer_tup_to_int((h_in, w_in,\n",
    "                                                               c_in),\n",
    "                                                              in_layer_shape)\n",
    "                                conv_weight_matrix[in_int][out_int] = weight\n",
    "\n",
    "    weights_array = [conv_weight_matrix]\n",
    "\n",
    "    if conv_dict['max_pool_after']:\n",
    "        k_height, k_width = conv_dict['max_pool_size']\n",
    "        stride = conv_dict['max_pool_size']  # conv_dict['max_pool_stride']\n",
    "        padding = conv_dict['max_pool_padding']\n",
    "\n",
    "        if max_weight_convention == 'all_one':\n",
    "            max_weight = 1\n",
    "        elif max_weight_convention == 'one_on_n':\n",
    "            max_weight = 1 / (k_height * k_width)\n",
    "        else:\n",
    "            raise ValueError(\"max_weight_convention must be 'one_on_n' or 'all_one', is instead\" + max_weight_convention)\n",
    "\n",
    "        # This code works on valid, I tested it\n",
    "        # But if input_side is divisible by stride, it is the same\n",
    "        if padding == 'valid':\n",
    "            maxed_height = math.ceil(in_height / stride[0])\n",
    "            maxed_width = math.ceil(in_width / stride[1])\n",
    "            maxed_shape = (maxed_height, maxed_width, n_chan_out)\n",
    "            maxed_size = np.product(maxed_shape)\n",
    "            max_matrix = np.zeros((out_layer_size, maxed_size))\n",
    "\n",
    "            k_height_centre = int((k_height - 1) / 2)\n",
    "            k_width_centre = int((k_width - 1) / 2)\n",
    "            \n",
    "            for i in range(maxed_height):\n",
    "                for j in range(maxed_width):\n",
    "                    for c in range(n_chan_out):\n",
    "                        max_int = cnn_layer_tup_to_int((i,j,c), maxed_shape)\n",
    "                        for n in range(k_height):\n",
    "                            for m in range(k_width):\n",
    "                                h_in = stride[0] * i + n - k_height_centre\n",
    "                                w_in = stride[1] * j + m - k_width_centre\n",
    "                                in_bounds_check = (h_in in range(in_height)\n",
    "                                                   and\n",
    "                                                   w_in in range(in_width))\n",
    "                                if in_bounds_check:\n",
    "                                    out_int = cnn_layer_tup_to_int(\n",
    "                                        (h_in, w_in, c), out_layer_shape\n",
    "                                    )\n",
    "                                    max_matrix[out_int][max_int] = max_weight\n",
    "\n",
    "        # originally, this was 'valid`, but I don't know what it is\n",
    "        # this code reaise an IndexError\n",
    "        elif padding == 'same':\n",
    "            raise NotImplementedError\n",
    "\n",
    "            maxed_height = math.ceil((in_height - k_height + 1) / stride[0])\n",
    "            maxed_width = math.ceil((in_width - k_width + 1) / stride[1])\n",
    "            maxed_shape = (maxed_height, maxed_width, n_chan_out)\n",
    "            maxed_size = np.product(maxed_shape)\n",
    "            max_matrix = np.zeros((out_layer_size, maxed_size))\n",
    "\n",
    "            for i in range(maxed_height):\n",
    "                for j in range(maxed_width):\n",
    "                    for c in range(n_chan_out):\n",
    "                        max_int = cnn_layer_tup_to_int((i,j,c), maxed_shape)\n",
    "                        for n in range(kernel_height):\n",
    "                            for m in range(kernel_width):\n",
    "                                h_in = stride[0] * i + n\n",
    "                                w_in = stride[1] * i + m\n",
    "                                out_int = cnn_layer_tup_to_int((h_in, w_in, c),\n",
    "                                                               out_layer_shape)\n",
    "                                max_matrix[out_int][max_int] = max_weight\n",
    "        else:\n",
    "            raise ValueError(\"invalid value for 'max_pool_padding'\")\n",
    "\n",
    "        weights_array.append(max_matrix)\n",
    "\n",
    "    return weights_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ddc740",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_to_graph(weights_array):\n",
    "    # take an array of weight matrices, and return the adjacency matrix of the\n",
    "    # neural network it defines.\n",
    "    # if the weight matrices are A, B, C, and D, the adjacency matrix should be\n",
    "    # [[0   A   0   0   0  ]\n",
    "    #  [A^T 0   B   0   0  ]\n",
    "    #  [0   B^T 0   C   0  ]\n",
    "    #  [0   0   C^T 0   D  ]\n",
    "    #  [0   0   0   D^T 0  ]]\n",
    "\n",
    "    block_mat = []\n",
    "\n",
    "    # for everything in the weights array, add a row to block_mat of the form\n",
    "    # [None, None, ..., sparsify(np.abs(mat)), None, ..., None]\n",
    "    for (i, mat) in enumerate(weights_array):\n",
    "        sp_mat = sparse.coo_matrix(np.abs(mat))\n",
    "        if i == 0:\n",
    "            # add a zero matrix of the right size to the start of the first row\n",
    "            # so that our final matrix is of the right size\n",
    "            n = mat.shape[0]\n",
    "            first_zeroes = sparse.coo_matrix((n, n))\n",
    "            block_row = [first_zeroes] + [None]*len(weights_array)\n",
    "        else:\n",
    "            block_row = [None]*(len(weights_array) + 1)\n",
    "        block_row[i+1] = sp_mat\n",
    "        block_mat.append(block_row)\n",
    "\n",
    "    # add a final row to block_mat that's just a bunch of [None]s followed by a\n",
    "    # zero matrix of the right size\n",
    "    m = weights_array[-1].shape[1]\n",
    "    final_zeroes = sparse.coo_matrix((m, m))\n",
    "    nones_row = [None]*len(weights_array)\n",
    "    nones_row.append(final_zeroes)\n",
    "    block_mat.append(nones_row)\n",
    "\n",
    "    # turn block_mat into a sparse matrix\n",
    "    up_tri = sparse.bmat(block_mat, 'csr')\n",
    "\n",
    "    # we now have a matrix that looks like\n",
    "    # [[0   A   0   0   0  ]\n",
    "    #  [0   0   B   0   0  ]\n",
    "    #  [0   0   0   C   0  ]\n",
    "    #  [0   0   0   0   D  ]]\n",
    "    # add this to its transpose to get what we want\n",
    "    adj_mat = up_tri + up_tri.transpose()\n",
    "    return adj_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b1b69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.load_saved_model(model_path)\n",
    "weights = []\n",
    "\n",
    "for layer in model.layers: \n",
    "    w = None\n",
    "    if 'conv' in layer.name: \n",
    "        w = conv_layer_to_weight_mats(layer.weights)\n",
    "    else if 'dense' in layer.name: \n",
    "        w = layer.weights\n",
    "    if w: \n",
    "        weights.append(w)\n",
    "weights_to_graph(weights)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
